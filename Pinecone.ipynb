{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcrDWYvSnGkz",
        "outputId": "5a7235bc-6a9b-4411-a089-9d962457ad02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/419.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m317.4/419.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install pinecone --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U voyageai --quiet"
      ],
      "metadata": {
        "id": "nVfTPqQApvlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY=\"66ab5bcf-69d9-4087-8327-bedc4a3e2978\""
      ],
      "metadata": {
        "id": "P1Cro0NbnenN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ],
      "metadata": {
        "id": "OGTHkyrboiAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import voyageai\n",
        "\n",
        "vo = voyageai.Client(api_key=\"pa-BST3zUVQrL0AGvxkMUSVKKVMX5iPxF278HIUVTwH734\")\n",
        "# Alternatively, you can use vo = voyageai.Client(api_key=\"<your secret key>\")\n",
        "\n",
        "result = vo.embed([\"hello world\"], model=\"voyage-law-2\")"
      ],
      "metadata": {
        "id": "EzVXBrAqoxZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xgCwPQp0Fgo",
        "outputId": "8e4b64e4-c769-494e-8b36-4f807f188f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KirMjdEAyJYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split(\"\\n\")[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oasFC4An-UaB",
        "outputId": "3a5a6f47-ebaf-48e2-aedd-8b163b34011a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Annex III, point (3)  Annex I, point (8)  ▼B'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(text):\n",
        "    embeddings = vo.embed(\n",
        "        texts=[text],\n",
        "        model='voyage-law-2',\n",
        "        input_type='document',\n",
        "        truncation=True\n",
        "    ).embeddings\n",
        "    return embeddings[0]"
      ],
      "metadata": {
        "id": "xJzmzu9_0jPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = generate_embeddings(text)"
      ],
      "metadata": {
        "id": "W0BhvaI85Yrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "index_name = \"laws\"\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "def upsert_to_pinecone(id, embedding, metadata):\n",
        "    index.upsert(vectors=[(id, embedding, metadata)])\n",
        "\n",
        "upsert_to_pinecone(\"1\", embeddings, {\"name\": \"EU AI ACT\"})"
      ],
      "metadata": {
        "id": "gZZpkpYW0tHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/GDPR Full Text.pdf\")\n",
        "embeddings = generate_embeddings(text)\n",
        "upsert_to_pinecone(\"2\", embeddings, {\"name\": \"GDPR Full Text\"})"
      ],
      "metadata": {
        "id": "_LbRNiwv5rxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/eu digital markets act.pdf\")\n",
        "embeddings = generate_embeddings(text)\n",
        "upsert_to_pinecone(\"3\", embeddings, {\"name\": \"EU Digital Markets Act\"})"
      ],
      "metadata": {
        "id": "XM2xgNV26DAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/network and information security directive.pdf\")\n",
        "embeddings = generate_embeddings(text)\n",
        "upsert_to_pinecone(\"4\", embeddings, {\"name\": \"Network and Information Security Directive\"})"
      ],
      "metadata": {
        "id": "T54m9EpU6XkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_text_splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "FTwDu8We_nAF",
        "outputId": "c5aa1b56-1c60-45d7-b356-f0c34b40239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_text_splitters\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain_text_splitters)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (4.12.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (2.32.3)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (2.23.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (2.2.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_text_splitters) (1.2.2)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, requests-toolbelt, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain_text_splitters\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.10 langchain_text_splitters-0.3.0 langsmith-0.1.134 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tenacity"
                ]
              },
              "id": "ce991960914a4546a79d2d6be6ba1431"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "g1OiZ_T9_lv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/GDPR Full Text.pdf\")"
      ],
      "metadata": {
        "id": "YiXy7ARPAQrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "# Split the text into chunks\n",
        "chunks = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "1QdKKrqW9KO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks, batch_size=128):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        embeddings = vo.embed(\n",
        "            texts=batch,\n",
        "            model='voyage-law-2',\n",
        "            input_type='document',\n",
        "            truncation=True\n",
        "        ).embeddings\n",
        "        all_embeddings.extend(embeddings)\n",
        "    return all_embeddings\n",
        "\n",
        "embeddings = embed_chunks(chunks)"
      ],
      "metadata": {
        "id": "OLYXSdV1A2sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings[0]"
      ],
      "metadata": {
        "id": "ZefdC9PHBEtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_title = \"GDPR Full Text\"\n",
        "upsert_data = []\n",
        "\n",
        "for idx, embedding in enumerate(embeddings):\n",
        "    vector_id = f\"{document_title}_{idx + 1}\"  # Create a unique ID for each chunk\n",
        "    metadata = {\n",
        "        \"name\": document_title,\n",
        "        \"chunk_id\": idx + 1,\n",
        "        \"text\": chunks[idx]  # Include the text of the chunk in metadata\n",
        "    }\n",
        "    upsert_data.append((vector_id, embedding, metadata))\n",
        "\n",
        "index.upsert(vectors=upsert_data)\n",
        "\n",
        "print(f\"Upserted {len(upsert_data)} vectors to Pinecone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YImBSkdNBxW3",
        "outputId": "06d36012-a1ee-4069-f5a9-28c12cdc03f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 455 vectors to Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/EU AI Act-201-459.pdf\")\n",
        "chunks = text_splitter.split_text(text)\n",
        "embeddings = embed_chunks(chunks)\n",
        "document_title = \"EU AI Act\"\n",
        "upsert_data = []\n",
        "\n",
        "for idx, embedding in enumerate(embeddings):\n",
        "    vector_id = f\"{document_title}_{idx + 1}\"  # Create a unique ID for each chunk\n",
        "    metadata = {\n",
        "        \"name\": document_title,\n",
        "        \"chunk_id\": idx + 1,\n",
        "        \"text\": chunks[idx]  # Include the text of the chunk in metadata\n",
        "    }\n",
        "    upsert_data.append((vector_id, embedding, metadata))\n",
        "\n",
        "index.upsert(vectors=upsert_data)\n",
        "\n",
        "print(f\"Upserted {len(upsert_data)} vectors to Pinecone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6UF_oGuCX4Q",
        "outputId": "0c438015-97e5-456b-e3b3-11c1b84eacba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 381 vectors to Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/eu digital markets act.pdf\")\n",
        "chunks = text_splitter.split_text(text)\n",
        "embeddings = embed_chunks(chunks)\n",
        "document_title = \"EU Digital Markets Act\"\n",
        "upsert_data = []\n",
        "\n",
        "for idx, embedding in enumerate(embeddings):\n",
        "    vector_id = f\"{document_title}_{idx + 1}\"  # Create a unique ID for each chunk\n",
        "    metadata = {\n",
        "        \"name\": document_title,\n",
        "        \"chunk_id\": idx + 1,\n",
        "        \"text\": chunks[idx]  # Include the text of the chunk in metadata\n",
        "    }\n",
        "    upsert_data.append((vector_id, embedding, metadata))\n",
        "\n",
        "index.upsert(vectors=upsert_data)\n",
        "\n",
        "print(f\"Upserted {len(upsert_data)} vectors to Pinecone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R_3B0AqEt1Z",
        "outputId": "a519a76e-6b9d-4072-a6fb-7cd9d6dba9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 332 vectors to Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = extract_text_from_pdf(\"/content/network and information security directive.pdf\")\n",
        "chunks = text_splitter.split_text(text)\n",
        "embeddings = embed_chunks(chunks)\n",
        "document_title = \"Network and Information Security Directive\"\n",
        "upsert_data = []\n",
        "\n",
        "for idx, embedding in enumerate(embeddings):\n",
        "    vector_id = f\"{document_title}_{idx + 1}\"  # Create a unique ID for each chunk\n",
        "    metadata = {\n",
        "        \"name\": document_title,\n",
        "        \"chunk_id\": idx + 1,\n",
        "        \"text\": chunks[idx]  # Include the text of the chunk in metadata\n",
        "    }\n",
        "    upsert_data.append((vector_id, embedding, metadata))\n",
        "\n",
        "index.upsert(vectors=upsert_data)\n",
        "\n",
        "print(f\"Upserted {len(upsert_data)} vectors to Pinecone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwTqlucZFAmu",
        "outputId": "1cce42bc-3b76-47bd-e7fe-2ff6aa0d4416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 185 vectors to Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"EU AI ACT\"\n",
        "\n",
        "# Generate embedding for the query\n",
        "query_embedding = vo.embed(texts=[query], model=\"voyage-law-2\", input_type=\"query\").embeddings[0]\n",
        "\n",
        "# Perform the search\n",
        "search_results = index.query(vector=query_embedding, top_k=3, include_metadata=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p0zDTaVKFc1N",
        "outputId": "db5cb3c0-0784-44b3-fbc7-2aabd83ad9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.592801273\n",
            "{'id': 'EU AI Act_21',\n",
            " 'metadata': {'chunk_id': 21.0,\n",
            "              'name': 'EU AI Act',\n",
            "              'summary': 'in line with the Union’s international trade '\n",
            "                         'commitments. They should also take into \\n'\n",
            "                         'account the Eu...'},\n",
            " 'score': 0.592801273,\n",
            " 'values': []}\n",
            "---\n",
            "Score: 0.589668\n",
            "{'id': 'EU AI Act_13',\n",
            " 'metadata': {'chunk_id': 13.0,\n",
            "              'name': 'EU AI Act',\n",
            "              'summary': 'fundamental rights obligations. Diverging national '\n",
            "                         'rules may lead to the fragmentation of \\n'\n",
            "                         'the inter...'},\n",
            " 'score': 0.589668,\n",
            " 'values': []}\n",
            "---\n",
            "Score: 0.587143362\n",
            "{'id': 'EU AI Act_23',\n",
            " 'metadata': {'chunk_id': 23.0,\n",
            "              'name': 'EU AI Act',\n",
            "              'summary': 'systems to benefit from the principle of free '\n",
            "                         'movement of goods and services. Those rules \\n'\n",
            "                         'should be...'},\n",
            " 'score': 0.587143362,\n",
            " 'values': []}\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query):\n",
        "    query_vector = vo.embed(texts=[query], model=\"voyage-law-2\", input_type=\"query\").embeddings[0]\n",
        "\n",
        "    # Find similar chunks\n",
        "    search_results = index.query(vector=query_vector, top_k=100, include_metadata=True)\n",
        "    relevant_chunks = [m.metadata['summary'] for m in search_results.matches]\n",
        "\n",
        "    # Combine the original query with the retrieved chunks\n",
        "    augmented_prompt = f\"\"\"\n",
        "Context:{' '.join(relevant_chunks)}\n",
        "\n",
        "User Question: {query}\n",
        "\n",
        "Response Guidelines:\n",
        "1. Begin with a brief summary of your understanding of the question.\n",
        "2. Analyze the provided context, citing relevant information as [Context: Document Name].\n",
        "3. If the context is insufficient, state \"Supplementing with online information\" and provide additional details, citing as [Online: Source Name].\n",
        "4. Present your answer in a structured format, using numbered or bulleted lists where appropriate.\n",
        "5. Include at least one relevant legal precedent or case study, if applicable.\n",
        "6. Address any potential counterarguments or alternative interpretations.\n",
        "7. Conclude with:\n",
        "   a) Confidence Level: (High/Medium/Low)\n",
        "   b) Key Takeaways: (2-3 bullet points)\n",
        "   c) Suggested Further Reading: (1-2 relevant sources)\"\"\"\n",
        "    # augmented_prompt = f\"Context:\\n{' '.join(relevant_chunks)}\"\n",
        "\n",
        "\n",
        "    return augmented_prompt\n",
        "\n",
        "# Example usage in your application\n",
        "user_query = \"What were the key arguments and results of the Meta Ireland vs DPC case?\"\n",
        "augmented_prompt = process_query(user_query)\n",
        "\n",
        "# Now you can send this augmented_prompt to your llama-3.1-sonar-small-128k-chat model via the Perplexity API"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ipojaQl-HHBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://api.perplexity.ai/chat/completions\"\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a sophisticated AI legal assistant with expertise in European tech law. Your task is to answer the following question using the provided context and, if necessary, your ability to search for current online information.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": process_query(user_query)\n",
        "        }\n",
        "    ],\n",
        "    \"temperature\": 0.2,\n",
        "    \"top_p\": 0.9,\n",
        "    \"return_citations\": True,\n",
        "    \"search_domain_filter\": [\"perplexity.ai\"],\n",
        "    \"return_images\": False,\n",
        "    \"return_related_questions\": False,\n",
        "    \"search_recency_filter\": \"month\",\n",
        "    \"top_k\": 0,\n",
        "    \"stream\": False,\n",
        "    \"presence_penalty\": 0,\n",
        "    \"frequency_penalty\": 1\n",
        "}\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer 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\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, json=payload, headers=headers)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "13KBPpkJLXPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "ast.literal_eval(response.text)['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "G_X39S6FNELW",
        "outputId": "41ab3f3e-9870-4b93-e246-711cd5cca5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**Summary of Understanding:**\\nThe question pertains to the key arguments and results of the Meta Ireland vs DPC case, focusing on the legal proceedings and the Data Protection Commission's (DPC) decision regarding Meta Ireland's handling of user passwords.\\n\\n**Analysis of Provided Context:**\\nThe context provided includes legal documents and news articles discussing the case. Key points include:\\n\\n- **Violation of GDPR:** Meta Ireland was fined €91 million by the DPC for violating the General Data Protection Regulation (GDPR) by storing user passwords in plaintext, failing to notify and document personal data breaches, and not using appropriate technical or organizational measures to ensure password security.\\n- **Legal Proceedings:** The case involves an appeal by Meta Ireland against the DPC's decision, with discussions on the approach to the appeal proceedings, including the possibility of a modular trial and the interpretation of Article 83 of the GDPR[Context: THE HIGH COURT 2022 339 MCA].\\n- **Previous Fines:** The DPC had previously fined Meta Ireland for other GDPR violations, including a €1.2 billion fine for unlawful data transfers to the US.\\n\\n**Supplementing with Online Information:**\\nNo additional online information is needed as the provided context sufficiently covers the key arguments and results of the case.\\n\\n**Structured Answer:**\\n\\n1. **Key Arguments:**\\n   - **Meta Ireland's Violations:** The DPC found that Meta Ireland violated the GDPR by storing user passwords in plaintext, failing to notify and document personal data breaches, and not using appropriate technical or organizational measures to ensure password security.\\n   - **Appeal Proceedings:** The case involves an appeal by Meta Ireland against the DPC's decision, with discussions on the approach to the appeal proceedings, including the possibility of a modular trial and the interpretation of Article 83 of the GDPR.\\n\\n2. **Results:**\\n   - **Fine:** Meta Ireland was fined €91 million by the DPC.\\n   - **Legal Precedent:** The case sets a precedent for the enforcement of GDPR regulations, particularly regarding the secure storage of user passwords and the notification of personal data breaches.\\n\\n3. **Legal Precedent/Case Study:**\\n   - **Relevant Case:** The case of WhatsApp's appeal against the DPC's decision, which also involved GDPR violations, provides a relevant legal precedent for understanding the enforcement of GDPR regulations in similar contexts[Context: THE COURT OF APPEAL Record Number: 2023/282].\\n\\n4. **Counterarguments/Alternative Interpretations:**\\n   - **Meta's Defense:** Meta Ireland argued that it took immediate action to fix the error and that there was no evidence that the passwords were abused or accessed improperly. However, the DPC emphasized the risk of abuse from storing passwords in plaintext.\\n\\n**Conclusion:**\\n\\n- **Confidence Level:** High\\n- **Key Takeaways:**\\n  - The DPC fined Meta Ireland €91 million for violating the GDPR by storing user passwords in plaintext and failing to notify and document personal data breaches.\\n  - The case sets a precedent for the enforcement of GDPR regulations regarding secure password storage and breach notification.\\n  - The appeal proceedings involve discussions on the approach to the appeal, including the possibility of a modular trial and the interpretation of Article 83 of the GDPR.\\n- **Suggested Further Reading:**\\n  - ** Irish Regulator Fines Meta €91 Million for Storing Passwords in Plaintext** (NatLawReview)\\n  - ** Meta fined €91m by DPC for password storage issues** (RTE)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Life"
      ],
      "metadata": {
        "id": "wKe82qd6Z-ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import voyageai\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "djIraMwGZ9yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(\"semantic-laws\")"
      ],
      "metadata": {
        "id": "IpZXAzv1audd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_chunking(text, chunk_size=1000, chunk_overlap=200, num_clusters=10):\n",
        "    # Initialize the text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "\n",
        "    # Split the text into initial chunks\n",
        "    initial_chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Generate embeddings for each chunk\n",
        "    vo = voyageai.Client(api_key=\"pa-BST3zUVQrL0AGvxkMUSVKKVMX5iPxF278HIUVTwH734\")\n",
        "    embeddings = vo.embed(\n",
        "        texts=initial_chunks,\n",
        "        model='voyage-law-2',\n",
        "        input_type='document',\n",
        "        truncation=True\n",
        "    ).embeddings\n",
        "\n",
        "    # Perform K-means clustering on the embeddings\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    # Group chunks by cluster\n",
        "    clustered_chunks = [[] for _ in range(num_clusters)]\n",
        "    for chunk, label in zip(initial_chunks, cluster_labels):\n",
        "        clustered_chunks[label].append(chunk)\n",
        "\n",
        "    # Merge chunks within each cluster\n",
        "    final_chunks = []\n",
        "    for cluster in clustered_chunks:\n",
        "        merged_chunk = \" \".join(cluster)\n",
        "        final_chunks.append(merged_chunk)\n",
        "\n",
        "    return final_chunks"
      ],
      "metadata": {
        "id": "TE5Ox_ouaFPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_chunks(chunks, batch_size=128):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i+batch_size]\n",
        "        embeddings = vo.embed(\n",
        "            texts=batch,\n",
        "            model='voyage-law-2',\n",
        "            input_type='document',\n",
        "            truncation=True\n",
        "        ).embeddings\n",
        "        all_embeddings.extend(embeddings)\n",
        "    return all_embeddings"
      ],
      "metadata": {
        "id": "OCgclu2CbKdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(pdf_path, document_title, chunk_size=500, chunk_overlap=50):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    embeddings = embed_chunks(chunks)\n",
        "\n",
        "    upsert_data = []\n",
        "    for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "        vector_id = f\"{document_title}_{idx + 1}\"\n",
        "\n",
        "        # Create a summary of the chunk (e.g., first 100 characters)\n",
        "        summary = chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
        "\n",
        "        metadata = {\n",
        "            \"name\": document_title,\n",
        "            \"chunk_id\": idx + 1,\n",
        "            \"summary\": summary  # Store a summary instead of full text\n",
        "        }\n",
        "\n",
        "        upsert_data.append((vector_id, embedding, metadata))\n",
        "\n",
        "    # Upsert in smaller batches to avoid hitting size limits\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(upsert_data), batch_size):\n",
        "        batch = upsert_data[i:i+batch_size]\n",
        "        index.upsert(vectors=batch)\n",
        "\n",
        "    print(f\"Upserted {len(upsert_data)} vectors to Pinecone for {document_title}.\")"
      ],
      "metadata": {
        "id": "0I7SXSzTaRe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process EU AI Act\n",
        "process_document(\"/content/EU AI Act-201-459.pdf\", \"EU AI Act\")\n",
        "process_document(\"/content/EU AI Act-1-200.pdf\", \"EU AI Act\")\n",
        "\n",
        "# Process GDPR\n",
        "process_document(\"/content/GDPR Full Text.pdf\", \"GDPR Full Text\")\n",
        "\n",
        "# Process EU Digital Markets Act\n",
        "process_document(\"/content/eu digital markets act.pdf\", \"EU Digital Markets Act\")\n",
        "\n",
        "# Process Network and Information Security Directive\n",
        "process_document(\"/content/network and information security directive.pdf\", \"Network and Information Security Directive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDoZyfsVaYJW",
        "outputId": "45870d6a-fbe4-44d1-c40d-a7dd6267b350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 668 vectors to Pinecone for EU AI Act.\n",
            "Upserted 658 vectors to Pinecone for EU AI Act.\n",
            "Upserted 838 vectors to Pinecone for GDPR Full Text.\n",
            "Upserted 611 vectors to Pinecone for EU Digital Markets Act.\n",
            "Upserted 330 vectors to Pinecone for Network and Information Security Directive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_document(\"/content/Co-ordinated Plan on AI - EU.pdf\", \"Plan on AI\")\n",
        "process_document(\"/content/EPRS_STU(2020)641530_EN.pdf\", \"EPRS_STU\")\n",
        "process_document(\"/content/Strategy on AI - EU.pdf\", \"Strategy on AI\")\n",
        "process_document(\"/content/White Paper On AI - EU.pdf\", \"White Paper\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6J2XPRC5dL8",
        "outputId": "ad7b4f6f-64d0-481f-8dc0-678b8a7c3b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 19 vectors to Pinecone for Plan on AI.\n",
            "Upserted 763 vectors to Pinecone for EPRS_STU.\n",
            "Upserted 144 vectors to Pinecone for Strategy on AI.\n",
            "Upserted 217 vectors to Pinecone for White Paper.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_document(\"/content/GenAI in Energy Workshop Report.pdf\",\"GenAI in Energy Workshop Report\")\n",
        "process_document(\"/content/Historic Timeline _ EU Artificial Intelligence Act.pdf\", \"Historic Timeline EU AI\")\n",
        "process_document(\"/content/Summary of 2023’s Key CJEU Data Protection Judgments - Arthur Cox LLP.pdf\", \"CJEU Data Protection\")\n",
        "process_document(\"/content/The Right to Compensation Under the GDPR Key Takeaways from Recent Case Law of the Court of Justice of the European Union -.pdf\", \"Right to Compensation GDPR\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "id": "NP5jz3OG6336",
        "outputId": "e0ff9ad2-8be8-469e-9099-29e1411104d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 19 vectors to Pinecone for GenAI in Energy Workshop Report.\n",
            "Upserted 13 vectors to Pinecone for Historic Timeline EU AI.\n",
            "Upserted 47 vectors to Pinecone for CJEU Data Protection.\n",
            "Upserted 35 vectors to Pinecone for Right to Compensation GDPR.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Co-ordinated Plan on AI - EU.pdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-63e717f0958f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Summary of 2023’s Key CJEU Data Protection Judgments - Arthur Cox LLP.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CJEU Data Protection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/The Right to Compensation Under the GDPR Key Takeaways from Recent Case Law of the Court of Justice of the European Union -.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Right to Compensation GDPR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Co-ordinated Plan on AI - EU.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Plan on AI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/node_12990_printable_pdf.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"node_12990\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/node_12991_printable_pdf.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"node_12991\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-76cc1a23d72d>\u001b[0m in \u001b[0;36mprocess_document\u001b[0;34m(pdf_path, document_title, chunk_size, chunk_overlap)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     text_splitter = RecursiveCharacterTextSplitter(\n\u001b[1;32m      5\u001b[0m         \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b7c156f41674>\u001b[0m in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Co-ordinated Plan on AI - EU.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_document(\"/content/node_12990_printable_pdf.pdf\", \"node_12990\")\n",
        "process_document(\"/content/node_12991_printable_pdf.pdf\", \"node_12991\")\n",
        "process_document(\"/content/node_13040_printable_pdf.pdf\", \"node_13040\")\n",
        "process_document(\"/content/node_13041_printable_pdf.pdf\", \"node_13041\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPB_doat8QCv",
        "outputId": "ca4c83e8-a93e-4fca-9238-2bb3056f8282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 11 vectors to Pinecone for node_12990.\n",
            "Upserted 7 vectors to Pinecone for node_12991.\n",
            "Upserted 20 vectors to Pinecone for node_13040.\n",
            "Upserted 7 vectors to Pinecone for node_13041.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_document(\"/content/Fox vs DPC 25 April 2024.pdf\", \"Fox vs DPC\")\n",
        "process_document(\"/content/Meta Ireland vs DPC 13 Feb 2024.pdf\", \"Meta Ireland vs DPC\")\n",
        "process_document(\"/content/Meta Platoforms Ireland LTD vs DTC 10 May 2024.pdf\", \"Meta Platforms Ireland LTD\")\n",
        "process_document(\"/content/Nowak vs Courts Service 20 March 2024.pdf\", \"Nowak vs Courts Service\")\n",
        "process_document(\"/content/Nowak vs DPC 2 July 2024.pdf\", \"Nowak vs DPC\")\n",
        "process_document(\"/content/Ryan vs DPC 24 Jun 2024.pdf\", \"Ryan vs DPC\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bHyW8Oi9tyD",
        "outputId": "d1c0cea9-e339-4798-8e9f-e8f9e95bc2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 129 vectors to Pinecone for Fox vs DPC.\n",
            "Upserted 18 vectors to Pinecone for Meta Ireland vs DPC.\n",
            "Upserted 77 vectors to Pinecone for Meta Platforms Ireland LTD.\n",
            "Upserted 57 vectors to Pinecone for Nowak vs Courts Service.\n",
            "Upserted 19 vectors to Pinecone for Nowak vs DPC.\n",
            "Upserted 173 vectors to Pinecone for Ryan vs DPC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fBsqqMli90vP"
      }
    }
  ]
}